{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from sklearn import cluster, preprocessing as pre\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_SET = {\n",
    "    \"amazon\": \"data/amazon_cells_labelled.txt\",\n",
    "    \"imdb\": \"data/imdb_labelled.txt\",\n",
    "    \"yelp\": \"data/yelp_labelled.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A. Parse data sets\n",
    "amazon = pd.read_csv(DATA_SET['amazon'], sep=\"\\t\", header=None, names=['Sentence', 'Label']).dropna()\n",
    "imdb = pd.read_csv(DATA_SET['imdb'], sep=\"\\t(?=[01])\", header=None, names=['Sentence', 'Label'], engine='python').dropna()\n",
    "yelp = pd.read_csv(DATA_SET['yelp'], sep=\"\\t\", header=None, names=['Sentence', 'Label']).dropna()\n",
    "\n",
    "parsed_data = pd.concat([amazon, imdb, yelp], ignore_index=True) # 3000 sentences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label_ratio (data_set):\n",
    "    print \"LABEL 0: \", sum(data_set['Label'] == 0)\n",
    "    print \"LABEL 1: \", sum(data_set['Label'] == 1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL 0:  1500\n",
      "LABEL 1:  1500\n"
     ]
    }
   ],
   "source": [
    "# Get counts of each label\n",
    "get_label_ratio(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# B. Preprocessing\n",
    "def preprocess (data):    \n",
    "    stopwords = set([\"the\", \"and\", \"or\", \"a\"]) # add some more later!\n",
    "    for i in range(len(data)):\n",
    "        data[i] = re.sub(\"[^a-zA-Z]\", \" \", data[i])\n",
    "        data[i] = data[i].lower().strip()\n",
    "        temp = [word for word in data[i].split() if word not in stopwords]\n",
    "        data[i] = \" \".join(temp)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "preprocess(parsed_data['Sentence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# C. Split training and testing data\n",
    "def split_data (data):\n",
    "    zeros = data.loc[data['Label'] == 0]\n",
    "    ones = data.loc[data['Label'] == 1]\n",
    "    train, test = pd.concat([zeros[:400], ones[:400]]), pd.concat([zeros[400:], ones[400:]])\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = {}, {}\n",
    "for k, data in parsed_data.items():\n",
    "    train[k], test[k] = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# D. Bag of words\n",
    "def get_bag (data):\n",
    "    bag = []\n",
    "    for sentence in data:\n",
    "        for word in sentence.split():\n",
    "            bag.append(word)\n",
    "    return np.unique(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BAGS = dict(zip(parsed_data.keys(), [get_bag(data['Sentence']) for k, data in train.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_vector (bag, data):\n",
    "    features = []\n",
    "    for sentence in data:\n",
    "        f = []\n",
    "        s = sentence.split()\n",
    "        for word in bag:\n",
    "            f.append(s.count(word))\n",
    "        features.append(f)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = {}\n",
    "LABELS = {}\n",
    "for k in BAGS:\n",
    "    features[k] = get_feature_vector (BAGS[k], parsed_data[k]['Sentence'].values)\n",
    "    LABELS[k] = parsed_data[k]['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# E. Postprocessing\n",
    "def post_process (data):\n",
    "    norm = []\n",
    "    for features in data:\n",
    "        mapped = map(float, features)\n",
    "        norm.append(pre.normalize(mapped, norm='l2').flatten())\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# e.g. access POST_PROCESSED['amazon'] or POST_PROCESSED['imdb']\n",
    "POST_PROCESSED = dict(zip(parsed_data.keys(), [post_process(features[k]) for k in parsed_data.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "#     def kmeans(self, X, k_clusters = 2, max_iterations=1000):\n",
    "    def kmeans(self, X, k_clusters, max_iterations):\n",
    "        centroids = self.get_initial_clusters(X, k_clusters)\n",
    "        old_centroids = []\n",
    "        \n",
    "        iteration = 0\n",
    "        while not np.array_equal(centroids, old_centroids) and iteration < max_iterations:\n",
    "            old_centroids = centroids[:] # must copy list, not assign\n",
    "            clusters = [[] for i in range(k_clusters)]\n",
    "            # cluster points to nearest centroid\n",
    "            for x in X:\n",
    "                min_dist = sys.maxint\n",
    "                kth_idx = -1\n",
    "                for idx, centroid in enumerate(centroids):\n",
    "                    dist = spatial.distance.euclidean(x, centroid)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        kth_idx = idx\n",
    "                clusters[kth_idx].append(x)\n",
    "            # update centroid\n",
    "#             for idx, centroid in enumerate(centroids):\n",
    "#                 centroids[idx] = np.array(clusters[idx]).sum(axis=0) / len(clusters[idx])\n",
    "            for idx, cluster in enumerate(clusters):\n",
    "                if cluster == []:\n",
    "                    raise Exception('Empty cluster, try different centroid initialization')\n",
    "                centroids[idx] = np.array(cluster).sum(axis=0) / float(len(cluster))\n",
    "            iteration += 1\n",
    "            \n",
    "        return centroids, clusters\n",
    "    # method 1\n",
    "    # sometimes bad initial clusters cause empty clusters when euclidean distance is taken\n",
    "    def get_initial_clusters(self, X, k_clusters):\n",
    "        centroids = []\n",
    "        for i in range(0, k_clusters):\n",
    "            centroids.append(X[np.random.randint(0, len(X), size=1)])\n",
    "        return centroids\n",
    "    # method 2\n",
    "    def get_initial_clusters2(self, X, k_clusters):\n",
    "        centroids = []\n",
    "        start_idx, end_idx = 0, len(X) / k_clusters\n",
    "        for i in range(0, k_clusters):\n",
    "            centroids.append(np.array(X[start_idx : end_idx]).sum(axis=0) / float(len(X) / k_clusters))\n",
    "            start_idx = end_idx\n",
    "            end_idx += end_idx\n",
    "        return centroids\n",
    "    \n",
    "    def get_neighbors(self, X):\n",
    "        neighbors = []\n",
    "        for idx1, point1 in enumerate(X): # for each matrix of pixels\n",
    "            distances = []\n",
    "            [distances.append((spatial.distance.euclidean(point1, point2), idx2)) for idx2, point2 in enumerate(X)]\n",
    "            distances.sort(key=itemgetter(0)) # sort list of tuples based on key 0, or distance!\n",
    "            nearest_neighbors = distances[1:self.k_neighbors+1] # remove 0 distance while comparing the same value\n",
    "            neighbors.append(nearest_neighbors)\n",
    "        return neighbors\n",
    "    \n",
    "    def classifier(self, nearest_neighbors, digit_labels):\n",
    "        possible_classes = []\n",
    "        for neighbor in nearest_neighbors:\n",
    "            possible_classes.append(digit_labels[neighbor[1]])\n",
    "\n",
    "        return max(set(possible_classes), key=possible_classes.count)\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        predicted_digits = []\n",
    "        for test_instance in test_data:\n",
    "            neighbors = self.get_neighbors(self.k_neighbors, self.train_data, test_instance)\n",
    "            predicted_digits.append(self.classifier(neighbors, self.labels))\n",
    "        return predicted_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = [[1,1,1,1,1], [2,2,2,2,2], [3,3,3,3,3], [4,4,4,4,4], [10,10,10,10,10], [1,534,3,64,1], [2343,2,245,2,5], [0,100,0,100,3], [4,4,4,4,4], [10,10,10,10,10]]\n",
    "centroids, clusters = KMeans().kmeans(X, 2, 1000000)\n",
    "\n",
    "k_means = cluster.KMeans(n_clusters=2)\n",
    "k_means.fit(X) \n",
    "values = k_means.cluster_centers_\n",
    "labels = k_means.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   4.66666667   15.11111111   15.11111111   31.          264.11111111]\n",
      " [   1.            1.            3.           64.          534.        ]]\n",
      "[[  3.88888889e+00   4.11111111e+00   4.22222222e+00   2.20000000e+01\n",
      "    7.42222222e+01]\n",
      " [  2.00000000e+00   2.00000000e+00   5.00000000e+00   2.45000000e+02\n",
      "    2.34300000e+03]]\n"
     ]
    }
   ],
   "source": [
    "print np.sort(np.array(centroids))\n",
    "print np.sort(values)\n",
    "# print clusters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids2, clusters2 = KMeans().kmeans(POST_PROCESSED['amazon'][:10], 2, 50000)\n",
    "\n",
    "k_means = cluster.KMeans(n_clusters=2)\n",
    "k_means.fit(POST_PROCESSED['amazon'][:10]) \n",
    "values2 = k_means.cluster_centers_\n",
    "labels2 = k_means.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   4.66666667   15.11111111   15.11111111   31.          264.11111111]\n",
      "[ 0.          0.          0.         ...,  0.12993231  0.19468914\n",
      "  0.40699761]\n"
     ]
    }
   ],
   "source": [
    "print np.sort(np.array(centroids[0]))\n",
    "print np.sort(values2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
