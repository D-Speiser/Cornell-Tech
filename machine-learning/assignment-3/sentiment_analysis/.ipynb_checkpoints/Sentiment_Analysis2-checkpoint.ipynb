{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, collections\n",
    "# brew install enchant, pip install pyenchant\n",
    "# import enchant.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_SET = {\n",
    "    \"amazon\": \"data/amazon_cells_labelled.txt\",\n",
    "    \"imdb\": \"data/imdb_labelled.txt\",\n",
    "    \"yelp\": \"data/yelp_labelled.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/pandas/io/parsers.py:648: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators; you can avoid this warning by specifying engine='python'.\n",
      "  ParserWarning)\n"
     ]
    }
   ],
   "source": [
    "# A. Parse data sets\n",
    "amazon = pd.read_csv(DATA_SET['amazon'], sep=\"\\t\", header=None, names=['Sentence', 'Label']).dropna()\n",
    "imdb = pd.read_csv(DATA_SET['imdb'], sep=\"\\t(?=[01])\", header=None, names=['Sentence', 'Label']).dropna()\n",
    "yelp = pd.read_csv(DATA_SET['yelp'], sep=\"\\t\", header=None, names=['Sentence', 'Label']).dropna()\n",
    "\n",
    "parsed_data = zip(DATA_SET.keys(), [amazon, imdb, yelp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label_ratio (data_set):\n",
    "    label_0 = sum(data_set['Label'] == 0)\n",
    "    label_1 = sum(data_set['Label'] == 1)\n",
    "    return (label_0, label_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMAZON\n",
      "Label 0:  500\n",
      "Label 1:  500 \n",
      "\n",
      "\n",
      "IMDB\n",
      "Label 0:  500\n",
      "Label 1:  500 \n",
      "\n",
      "\n",
      "YELP\n",
      "Label 0:  500\n",
      "Label 1:  500 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print ratio of different labels per data set\n",
    "for k, v in parsed_data:\n",
    "    label_0, label_1 = get_label_ratio(v)\n",
    "    print k.upper()\n",
    "    print \"Label 0: \", label_0\n",
    "    print \"Label 1: \", label_1, \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # http://norvig.com/spell-correct.html\n",
    "# def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "\n",
    "# def train(features):\n",
    "#     model = collections.defaultdict(lambda: 1)\n",
    "#     for f in features:\n",
    "#         model[f] += 1\n",
    "#     return model\n",
    "\n",
    "# NWORDS = train(words(file('data/big.txt').read()))\n",
    "\n",
    "# alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "# def edits1(word):\n",
    "#    splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "#    deletes    = [a + b[1:] for a, b in splits if b]\n",
    "#    transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "#    replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "#    inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
    "#    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "# def known_edits2(word):\n",
    "#     return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "# def known(words): return set(w for w in words if w in NWORDS)\n",
    "\n",
    "# def correct(word):\n",
    "#     candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "#     return max(candidates, key=NWORDS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper methods\n",
    "# def lower (df):\n",
    "#     return df.str.lower()\n",
    "\n",
    "# def replace (df, replace_dict):\n",
    "#     return df.replace(replace_dict, regex=True)\n",
    "\n",
    "replace_dict = {' the ':' ', ' and ':' ', ' or ':' ', '[^\\w\\s]':''}\n",
    "def preprocess(data_set):\n",
    "    for df in data_set:\n",
    "#     df[1]['Sentence'] = lower(df[1]['Sentence'])\n",
    "#     df[1]['Sentence'] = replace(df[1]['Sentence'], replace_dict)\n",
    "        df[1]['Sentence'] = df[1]['Sentence'].str.lower().replace(replace_dict, regex=True)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bag(df):\n",
    "    bag = []\n",
    "    for row in df:\n",
    "        for word in row.split():\n",
    "            bag.append(word)\n",
    "    return np.unique(bag)\n",
    "\n",
    "def bags_of_words(data_set):\n",
    "    bags = []\n",
    "    for df in data_set:\n",
    "        bags.append(get_bag(df[1]['Sentence']))\n",
    "    return {'amazon':bags[0], 'imdb':bags[1], 'yelp':bags[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed_data = preprocess(parsed_data)\n",
    "\n",
    "BAGS = bags_of_words(parsed_data)\n",
    "# test view bags\n",
    "# for word in BAGS['amazon']:\n",
    "#     print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '10', '100', ..., 'your', 'z500a', 'zero'], \n",
       "      dtype='|S22')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BAGS['amazon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1904"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_feature_vectors ():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
